# ğŸ¤— HuggingFace LLM Integration (LangChain + Streamlit)

This project demonstrates how to use **HuggingFace-hosted LLMs** via LangChain in a local app, enabling chat, code generation, and basic NLP tasks. The app uses a Streamlit UI and connects with public or private HuggingFace models.

---

## ğŸ“‚ Project Structure

```
Huggingface with Langchain/
â”œâ”€â”€ app.py               # Streamlit interface for chat/code prompts
â”œâ”€â”€ experiments.ipynb    # Notebook for testing various models/prompts
â””â”€â”€ requirements.txt     # Python dependencies
```

---

## ğŸš€ Features

- Integrates LangChain with **HuggingFace Inference API**
- Easily switch between text-generation models (e.g., Falcon, BLOOM, LLaMA)
- Chat-like prompt interface via Streamlit
- Experimental playground via Jupyter Notebook

---

## ğŸ› ï¸ How to Run

```bash
pip install -r requirements.txt
streamlit run app.py
```

> ğŸ” Add your HuggingFace API key as an environment variable or `.env` file if needed.

---

## ğŸ’¬ Sample Prompts

- â€œExplain how transformers work.â€
- â€œGenerate Python code to scrape a website.â€
- â€œSummarize the following textâ€¦â€

---

## ğŸ”­ Future Improvements

- Add dropdown to select models dynamically
- Improve prompt templates with chains
- Add token usage and latency stats

---



## ğŸ“« Contact

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Aparna-blue?style=flat&logo=linkedin)](https://www.linkedin.com/in/aparna-k-628005167/)
